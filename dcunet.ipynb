{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAPS89OQCOS2"
   },
   "source": [
    "The task completed by [Abapolov Philipp](https://vk.com/pheepa) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUzzX1lpCOS3"
   },
   "source": [
    "![header](img/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oX5DLS3COS5"
   },
   "source": [
    "# Phase-aware speech enchancement with Deep Complex U-Net #\n",
    "For research I chose this article [\"Phase-aware speech enchancement with Deep Complex U-Net\"](https://openreview.net/pdf?id=SkeRTsAcYm), which describes the architecture and training of a convolutional neural network for improving speech, so-called denoising, and set up an experiment. \n",
    "![architecture](img/dcunet.png)\n",
    "\n",
    "## The task ##\n",
    "The main task is to develop a complex variation of the architecture of the well known UNet network to eliminate unwanted noise from the audio.\n",
    "\n",
    "## Method's features ##\n",
    "Its speciality and difference from other networks, such as SegNet, for semantic segmentation (not only that) lies in Skip-Connections and that the values of both input data and all of network parameters (convolution filters, etc.) are complex.\n",
    "\n",
    "### Skip-Connections ###\n",
    "The main idea is that the early layers of the Encoder are concatenated with the \" parallel \" layers of the Decoder.\n",
    "\n",
    "![skip-connection](img/skip-connection.png)\n",
    "\n",
    "### Mask ###\n",
    "As a result of the convolution layers, we get a mask, which we multiply by the input time-frequency signal with noise and get a cleared time-frequency signal, which then passes the inverse Short-time Fourier transform.\n",
    "![arch](img/arch.png) \n",
    "\n",
    "\n",
    "### Alternative solution ###\n",
    "* [Improved Speech Enhancement with the Wave-U-Net](https://arxiv.org/abs/1811.11307)\n",
    "\n",
    "## The Experiment ##\n",
    "For training we will use [Noisy speech database for training speech enhancement algorithms and TTS models](https://datashare.is.ed.ac.uk/handle/10283/2791), which contains a data set for training and testing with 28 and 56 speakers in .wav audio files are 48 KHz. The 10-layer network architecture will be implemented, which looks like this:\n",
    "![10-layers](img/layers.png)\n",
    "\n",
    "A graph of changes in the value of the loss function during training and validation will be shown. \n",
    "\n",
    "The PESQ metric will also be calculated.\n",
    "\n",
    "### Issues ###\n",
    "Due to my lack of equipment with proper GPU (a laptop with 2 GB of GPU, so the model does not fit into the given memory, not to mention training) I had to consider alternatives for training:\n",
    "\n",
    "* Training on Google Colab or another cloud service. \n",
    "Cloud services have strict session time limits, so it was decided to train on a small number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiXUioRtCOS6"
   },
   "source": [
    "## Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0bWtt2J5i9F",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:03.964631Z",
     "start_time": "2024-05-19T13:53:02.639455Z"
    }
   },
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import copy\n",
    "\n",
    "from pesq import pesq\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# not everything is smooth in sklearn, to conveniently output images in colab\n",
    "# we will ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from IPython.display import clear_output"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xnlgTsICOTA"
   },
   "source": [
    "Checking whether the GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "byUnPtQ25i9O",
    "outputId": "91548102-17b7-42a1-896b-7f8b52f4d3ae",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:03.968268Z",
     "start_time": "2024-05-19T13:53:03.965881Z"
    }
   },
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "       \n",
    "DEVICE = torch.device('cuda' if train_on_gpu else 'cpu')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "qacrfNwA6vw_",
    "outputId": "83374e21-bed4-448a-b7a2-5791139f5375",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:04.093866Z",
     "start_time": "2024-05-19T13:53:03.968993Z"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JELiNbPtCOTH"
   },
   "source": [
    "# Loading and creating dataset objects #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YHdvu2FCRkA"
   },
   "source": [
    "### Loading ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoZKpT7iCojJ",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:04.335540Z",
     "start_time": "2024-05-19T13:53:04.095825Z"
    }
   },
   "source": [
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/2791/clean_trainset_28spk_wav.zip?sequence=2&isAllowed=y\n",
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/2791/noisy_trainset_28spk_wav.zip?sequence=6&isAllowed=y"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZI0BHsnDCVIi",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:04.580689Z",
     "start_time": "2024-05-19T13:53:04.336765Z"
    }
   },
   "source": [
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/2791/clean_testset_wav.zip?sequence=1&isAllowed=y\n",
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/2791/noisy_testset_wav.zip?sequence=5&isAllowed=y"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxkIsMa20pxp",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.058580Z",
     "start_time": "2024-05-19T13:53:04.581702Z"
    }
   },
   "source": [
    "!unzip clean_trainset_28spk_wav.zip?sequence=2.3\n",
    "!unzip noisy_trainset_28spk_wav.zip?sequence=6\n",
    "!unzip clean_testset_wav.zip?sequence=1\n",
    "!unzip nnoisy_testset_wav.zip?sequence=5"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDW1Vj2W05zX",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.185323Z",
     "start_time": "2024-05-19T13:53:05.059716Z"
    }
   },
   "source": [
    "!ls"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWvlIABzCOTM"
   },
   "source": [
    "The sampling frequency and the selected values for the Short-time Fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ngFJtPj5i9V",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.188970Z",
     "start_time": "2024-05-19T13:53:05.186277Z"
    }
   },
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = SAMPLE_RATE * 64 // 1000 + 4\n",
    "HOP_LENGTH = SAMPLE_RATE * 16 // 1000 + 4"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8suREWkb5Se"
   },
   "source": [
    "### The declaration of datasets and dataloaders ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZ0wb9EN5i9f",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.194805Z",
     "start_time": "2024-05-19T13:53:05.189655Z"
    }
   },
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class with audio that cuts them/paddes them to a specified length, applies a Short-tome Fourier transform,\n",
    "    normalizes and leads to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, noisy_files, clean_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        # list of files\n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.clean_files = sorted(clean_files)\n",
    "        \n",
    "        # stft parameters\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.len_ = len(self.noisy_files)\n",
    "        \n",
    "        # fixed len\n",
    "        self.max_len = 165000\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        return waveform\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # load to tensors and normalization\n",
    "        x_clean = self.load_sample(self.clean_files[index])\n",
    "        x_noisy = self.load_sample(self.noisy_files[index])\n",
    "        \n",
    "        # padding/cutting\n",
    "        x_clean = self._prepare_sample(x_clean)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "        \n",
    "        # Short-time Fourier transform\n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True)\n",
    "        x_clean_stft = torch.stft(input=x_clean, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return x_noisy_stft, x_clean_stft\n",
    "        \n",
    "    def _prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        current_len = waveform.shape[1]\n",
    "        \n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        output[0, -current_len:] = waveform[0, :self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "        \n",
    "        return output"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLV2lcv5i9k",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.199102Z",
     "start_time": "2024-05-19T13:53:05.197077Z"
    }
   },
   "source": [
    "TRAIN_NOISY_DIR = Path('noisy_trainset_28spk_wav')\n",
    "TRAIN_CLEAN_DIR = Path('clean_trainset_28spk_wav')\n",
    "\n",
    "TEST_NOISY_DIR = Path('noisy_testset_wav')\n",
    "TEST_CLEAN_DIR = Path('clean_testset_wav')"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBp3ws355i9o",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.202504Z",
     "start_time": "2024-05-19T13:53:05.199864Z"
    }
   },
   "source": [
    "train_noisy_files = sorted(list(TRAIN_NOISY_DIR.rglob('*.wav')))\n",
    "train_clean_files = sorted(list(TRAIN_CLEAN_DIR.rglob('*.wav')))\n",
    "\n",
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GHaKjYS5i9u",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.205221Z",
     "start_time": "2024-05-19T13:53:05.203236Z"
    }
   },
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_noisy_files, train_clean_files, N_FFT, HOP_LENGTH)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79nIQjht5i9y",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.361981Z",
     "start_time": "2024-05-19T13:53:05.206098Z"
    }
   },
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k73YEkgQCOTj"
   },
   "source": [
    "# Declaring the class layers #"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Znx7QM3h5i92",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.364207Z",
     "start_time": "2024-05-19T13:53:05.363867Z"
    }
   },
   "source": [
    "class CConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgtxJbSQ5i96",
    "ExecuteTime": {
     "end_time": "2024-05-19T13:53:05.365339Z",
     "start_time": "2024-05-19T13:53:05.365264Z"
    }
   },
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    \"\"\"\n",
    "      Class of complex valued dilation convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJSmVrxp5i9-"
   },
   "source": [
    "class CBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued batch normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7W37XMO5i-B"
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of upsample block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuugYDZs5i-G"
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of downsample block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconvt(x)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HO3p2zrOcn_z"
   },
   "source": [
    "10-layer model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8XCrVIg5i-K"
   },
   "source": [
    "class DCUnet10(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Complex U-Net class of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # downsampling/encoding\n",
    "        self.downsample0 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45)\n",
    "        self.downsample1 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=45, out_channels=90)\n",
    "        self.downsample2 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90)\n",
    "        self.downsample3 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90)\n",
    "        self.downsample4 = Encoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90)\n",
    "        \n",
    "        # upsampling/decoding\n",
    "        self.upsample0 = Decoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90)\n",
    "        self.upsample1 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90)\n",
    "        self.upsample2 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90)\n",
    "        self.upsample3 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=180, out_channels=45)\n",
    "        self.upsample4 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=90, output_padding=(0,1),\n",
    "                                 out_channels=1, last_layer=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, is_istft=True):\n",
    "        # downsampling/encoding\n",
    "        d0 = self.downsample0(x)\n",
    "        d1 = self.downsample1(d0) \n",
    "        d2 = self.downsample2(d1)        \n",
    "        d3 = self.downsample3(d2)        \n",
    "        d4 = self.downsample4(d3)\n",
    "        \n",
    "        # upsampling/decoding \n",
    "        u0 = self.upsample0(d4)\n",
    "        # skip-connection\n",
    "        c0 = torch.cat((u0, d3), dim=1)\n",
    "        \n",
    "        u1 = self.upsample1(c0)\n",
    "        c1 = torch.cat((u1, d2), dim=1)\n",
    "        \n",
    "        u2 = self.upsample2(c1)\n",
    "        c2 = torch.cat((u2, d1), dim=1)\n",
    "        \n",
    "        u3 = self.upsample3(c2)\n",
    "        c3 = torch.cat((u3, d0), dim=1)\n",
    "        \n",
    "        u4 = self.upsample4(c3)\n",
    "        \n",
    "        # u4 - the mask\n",
    "        output = u4 * x\n",
    "        if is_istft:\n",
    "            output = torch.squeeze(output, 1)\n",
    "            output = torch.istft(output, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3G5CqR3-COT8"
   },
   "source": [
    "## Loss function ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J71ny6expQeW"
   },
   "source": [
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "    # to time-domain waveform\n",
    "    y_true_ = torch.squeeze(y_true_, 1)\n",
    "    x_ = torch.squeeze(x_, 1)\n",
    "    y_true = torch.istft(y_true_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "    x = torch.istft(x_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "\n",
    "    y_pred = y_pred_.flatten(1)\n",
    "    y_true = y_true.flatten(1)\n",
    "    x = x.flatten(1)\n",
    "\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "    # true and estimated noise\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYvWz_6jRZ3e"
   },
   "source": [
    "Description of the training of epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VukJTCGIZ8ZU"
   },
   "source": [
    "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    train_ep_loss = 0.\n",
    "    counter = 0\n",
    "    for noisy_x, clean_x in train_loader:\n",
    "\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "\n",
    "        # zero  gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_ep_loss += loss.item() \n",
    "        counter += 1\n",
    "\n",
    "    train_ep_loss /= counter\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return train_ep_loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYCJWaTMRgS3"
   },
   "source": [
    "Description of the validation of epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JKrTMpPhw19"
   },
   "source": [
    "def test_epoch(net, test_loader, loss_fn):\n",
    "    net.eval()\n",
    "    test_ep_loss = 0.\n",
    "    counter = 0.\n",
    "    for noisy_x, clean_x in test_loader:\n",
    "        # get the output from the model\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        test_ep_loss += loss.item() \n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    test_ep_loss /= counter\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_ep_loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "879vq_uBRm_2"
   },
   "source": [
    "To understand whether the network is being trained or not, we will output a train and test loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4gdVmhRr1Qi"
   },
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "\n",
    "        # first evaluating for comparison\n",
    "        if e == 0:\n",
    "            with torch.no_grad():\n",
    "                test_loss = test_epoch(net, test_loader, loss_fn)\n",
    "                \n",
    "            test_losses.append(test_loss)\n",
    "            print(\"Loss before training:{:.6f}\".format(test_loss))\n",
    "          \n",
    "\n",
    "        train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        scheduler.step()\n",
    "        with torch.no_grad():\n",
    "          test_loss = test_epoch(net, test_loader, loss_fn)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Loss: {:.6f}...\".format(train_loss),\n",
    "                      \"Test Loss: {:.6f}\".format(test_loss))\n",
    "    return train_losses, test_losses"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHJZXeUQcsrq"
   },
   "source": [
    "## Metrics ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gDZAOnGdeB0"
   },
   "source": [
    "def pesq_score(net, test_loader):\n",
    "    net.eval()\n",
    "    test_pesq = 0.\n",
    "    counter = 0.\n",
    "\n",
    "\n",
    "    for noisy_x, clean_x in test_loader:\n",
    "        # get the output from the model\n",
    "        noisy_x = noisy_x.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            pred_x = net(noisy_x)\n",
    "        clean_x = torch.squeeze(clean_x, 1)\n",
    "        clean_x = torch.istft(clean_x, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "        \n",
    "        \n",
    "        psq = 0.\n",
    "        for i in range(len(clean_x)):\n",
    "            clean_x_16 = torchaudio.transforms.Resample(48000, 16000)(clean_x[i, 0,:].view(1,-1))\n",
    "            pred_x_16 = torchaudio.transforms.Resample(48000, 16000)(pred_x[i, 0,:].view(1,-1))\n",
    "\n",
    "            clean_x_16 = clean_x_16.cpu().cpu().numpy()\n",
    "            pred_x_16 = pred_x_16.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            psq += pesq(clean_x_16.flatten(), pred_x_16.flatten(), 16000)\n",
    "            \n",
    "        psq /= len(clean_x)\n",
    "        test_pesq += psq\n",
    "        counter += 1\n",
    "\n",
    "    test_pesq /= counter \n",
    "    return test_pesq"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6bIE9iOj8pq"
   },
   "source": [
    "## Training ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyBc1awQkI-D"
   },
   "source": [
    "# clear cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DIhn5cn85i-O"
   },
   "source": [
    "dcunet10 = DCUnet10(N_FFT, HOP_LENGTH).to(DEVICE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PR7dL8sJ5i-h"
   },
   "source": [
    "loss_fn = wsdr_fn\n",
    "optimizer = torch.optim.Adam(dcunet10.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "ppXkJUsY55vI",
    "outputId": "260b6225-8621-4927-9702-093155e0c983"
   },
   "source": [
    "train_losses, test_losses = train(dcunet10, train_loader, test_loader, loss_fn, optimizer, scheduler, 3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.save(dcunet10.state_dict(), '__weights.pth')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5R43OUHZYbKm"
   },
   "source": [
    "## PESQ метрика ##\n",
    "Let's calculate the average value of the metric in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bu3CHNxdYoKR"
   },
   "source": [
    "pesq_metric = pesq_score(dcunet10, test_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6rD0yrSbx1i"
   },
   "source": [
    "# The Results #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mw7_CKy2WeI_"
   },
   "source": [
    "## Loss ##\n",
    "Let's look at the graph of loss reduction for 3 epochs on the test, the loss for the first epoch decreases very quickly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "wCf59OBzoUon",
    "outputId": "e66330cc-a9b9-42fb-b287-b0258f990cc9"
   },
   "source": [
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.grid()\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VMGZjcpGorvU",
    "outputId": "7f016842-4e28-4f1f-bf40-76d077b5347f"
   },
   "source": [
    "print(\"Min test loss: {:.6f}, min train loss: {:.6f}\".format(min(test_losses), min(train_losses)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IRe-Iv84orJU"
   },
   "source": [
    "We managed to achieve a great loss, taking into account that the values lie on the segment [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLmET_EupdT_"
   },
   "source": [
    "## PESQ metrics ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0bjGE7-7pgAJ",
    "outputId": "3cfa6a99-e103-4d6b-8a41-ccd8dab4b2d6"
   },
   "source": [
    "print(\"Value of PESQ: {:.6f}\".format(pesq_metric))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vaApoDWpqGN"
   },
   "source": [
    "Values of the PESQ metric calculated on different networks from the article: \n",
    "\n",
    "![pesq](img/metric.png)\n",
    "\n",
    "Taking into account the fact that our 10-layer model was trained for only 2 epochs, we have achieved decent results. Our moel even shows a greater metric value than Wiener, SALON, MEGAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nvZcN3WYpmb"
   },
   "source": [
    "## Visualization ##\n",
    "Let's take a look at the time-domain waveform of a pure signal, with noise and predicted :\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhZKPirKvO3q"
   },
   "source": [
    "dcunet10.eval()\n",
    "x_n, x_c = iter(test_loader).next()\n",
    "x_est = dcunet10(x_n.cuda(), is_istft=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWySU5oOXraV"
   },
   "source": [
    "Purified"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "156VyDi1vOCe",
    "outputId": "8f80a35a-b4dd-492a-c97f-f768b154294c"
   },
   "source": [
    "plt.plot(x_est[1].view(-1).detach().cpu().numpy())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jBeTpGiXukt"
   },
   "source": [
    "Clear"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "Hh25gt1DvaO_",
    "outputId": "5a1aabae-f7ec-4e09-faca-10a44d61e665"
   },
   "source": [
    "x_c[1] = torch.squeeze(x_c[1], 1)\n",
    "plt.plot(torch.istft(x_c[1], n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgcJTi6sXwE1"
   },
   "source": [
    "With Noise"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "6xZ7N06evc7u",
    "outputId": "7670ecd6-d269-430e-ee8a-74513651cee4"
   },
   "source": [
    "x_n[1] = torch.squeeze(x_n[1], 1)\n",
    "plt.plot(torch.istft(x_n[1], n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFdga4o-X-yo"
   },
   "source": [
    "In General, it can be seen that the cleared signal is close to the purified one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qi7uvNVWqsvO"
   },
   "source": [
    "## Conclusions ##\n",
    "\n",
    "We conducted an experiment: we implemented the Deep Complex UNet architecture, trained it, and tested it.\n",
    "\n",
    "The obtained minimum loss value on validation: -0.976848\n",
    "The resulting PESQ metric value: 2.818076\n",
    "\n",
    "The value of the PESQ metric on a 20-layer network from the article: 3.24\n",
    "\n",
    "We were able to get close enough to the results of the study described in [the article](https://openreview.net/pdf?id=SkeRTsAcYm). But the values still turned out a little worse for the following reasons\n",
    "\n",
    "1.The possile minimum size of network: 10 layers instead of 20.\n",
    "\n",
    "2. A small number of epochs. 3 epochs is too little, but the results are enough for us.\n",
    "\n",
    "\n",
    "## The prospects ##\n",
    "* You can implement a deeper variation of Deep Complex U-Net using the same principle\n",
    "* The use of more powerful in terms of computing hardware(more memory) and increase the size of the batch to increase the speed of learning\n",
    "*Training on a larger number of epochs.\n",
    "* Augmentation of a dataset to increase its size by adding synthetic noise\n",
    "\n",
    "## In practice ##\n",
    "This technology can be used to improve the sound of phone calls, in the same voice messages in messengers and social networks, for professional video and audio processing.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "dcunet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
